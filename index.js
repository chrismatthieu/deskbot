const { Cam } = require('onvif');
const ollama = require('ollama').default;
const sharp = require('sharp');
const fs = require('fs');
const path = require('path');
const { spawn } = require('child_process');
const record = require('node-record-lpcm16');

// Camera configuration
const cameraConfig = {
  hostname: '192.168.0.42',
  username: 'admin',
  password: 'V1ctor1a',
  port: 80
};

// AI Vision configuration
const AI_CONFIG = {
  model: 'qwen2.5vl:3b',
  analysisInterval: 1000, // Analyze every 5 seconds
  confidenceThreshold: 0.7,
  maxRetries: 3
};

// Voice processing configuration
const VOICE_CONFIG = {
  sampleRate: 16000,
  channels: 1,
  threshold: 0.1, // Audio level threshold for voice detection
  silenceTimeout: 2000, // Stop recording after 2 seconds of silence
  recordingDuration: 5000 // Maximum recording duration in ms
};

let rtspStream = null;
let visionAnalysisActive = false;
let lastAnalysisTime = 0;
let aiAnalysisInProgress = false;
let gestureInProgress = false;
let voiceRecordingActive = false;
let currentAudioStream = null;

console.log('üîç Connecting to Amcrest camera...');
console.log(`üìç IP: ${cameraConfig.hostname}`);
console.log(`üë§ Username: ${cameraConfig.username}`);
console.log('ü§ñ Initializing AI Vision with Ollama...');

// Initialize Ollama connection
async function initializeOllama() {
  try {
    console.log('üß† Checking Ollama connection...');
    const models = await ollama.list();
    console.log('üìã Available models:', models.models.map(m => m.name));
    
    // Check if qwen2.5vl:3b is available
    const hasQwenModel = models.models.some(m => m.name.includes('qwen2.5vl:3b'));
    if (!hasQwenModel) {
      console.log('‚ö†Ô∏è  qwen2.5vl:3b model not found. Please install it with: ollama pull qwen2.5vl:3b');
      console.log('üîÑ Falling back to llama3.2-vision model...');
      AI_CONFIG.model = 'llama3.2-vision';
    } else {
      console.log('‚úÖ qwen2.5vl:3b model found!');
    }
    
    return true;
  } catch (error) {
    console.error('‚ùå Failed to connect to Ollama:', error.message);
    console.log('üí° Make sure Ollama is running: ollama serve');
    return false;
  }
}

// Capture frame from RTSP stream and convert to base64
async function captureFrame(rtspUrl) {
  return new Promise((resolve, reject) => {
    try {
      const outputPath = './temp_frame.jpg';
      
      // Use ffmpeg to capture a single frame from RTSP stream
      const ffmpegArgs = [
        '-i', rtspUrl,
        '-vframes', '1',
        '-f', 'image2',
        '-y', // Overwrite output file
        outputPath
      ];
      
      console.log('      üìπ Capturing frame with ffmpeg...');
      
      const ffmpeg = spawn('ffmpeg', ffmpegArgs, {
        stdio: ['ignore', 'pipe', 'pipe']
      });
      
      let stderr = '';
      
      ffmpeg.stderr.on('data', (data) => {
        stderr += data.toString();
      });
      
      ffmpeg.on('close', async (code) => {
        if (code === 0 && fs.existsSync(outputPath)) {
          try {
            // Use sharp to process the image and convert to base64
            const imageBuffer = await sharp(outputPath)
              .resize(640, 480, { fit: 'inside' }) // Resize for better performance
              .jpeg({ quality: 80 })
              .toBuffer();
            
            const base64String = imageBuffer.toString('base64');
            
            // Clean up
            fs.unlinkSync(outputPath);
            
            console.log('      ‚úÖ Frame captured and processed successfully');
            resolve(base64String);
          } catch (error) {
            console.error('      ‚ùå Error processing image:', error.message);
            reject(error);
          }
        } else {
          console.error('      ‚ùå FFmpeg failed:', stderr);
          reject(new Error(`FFmpeg failed with code ${code}: ${stderr}`));
        }
      });
      
      ffmpeg.on('error', (error) => {
        console.error('      ‚ùå FFmpeg error:', error.message);
        reject(error);
      });
      
    } catch (error) {
      reject(error);
    }
  });
}

// Analyze image with Ollama Vision model
async function analyzeImage(imageBase64) {
  try {
    console.log('üîç Analyzing image with AI...');
    
    const response = await ollama.chat({
      model: AI_CONFIG.model,
      messages: [
        {
          role: 'system',
          content: 'You are a helpful AI assistant that analyzes security camera footage. Describe what you see in a clear, concise manner using no more than 6 words. Focus on people, objects, activities, and any potential security concerns. Keep descriptions brief but informative.'
        },
        {
          role: 'user',
          content: 'What do you see in this security camera image?',
          images: [imageBase64]
        }
      ],
      stream: false
    });
    
    return response.message.content;
  } catch (error) {
    console.error('‚ùå AI analysis failed:', error.message);
    return null;
  }
}

// Start AI vision analysis
async function startVisionAnalysis(rtspUrl) {
  if (visionAnalysisActive) {
    console.log('‚ö†Ô∏è  Vision analysis already active');
    return;
  }
  
  visionAnalysisActive = true;
  console.log('üëÅÔ∏è  Starting AI Vision Analysis...');
  console.log(`üìä Analysis interval: ${AI_CONFIG.analysisInterval}ms`);
  
  const analysisLoop = async () => {
    if (!visionAnalysisActive) return;
    
    // Prevent overlapping AI requests
    if (aiAnalysisInProgress) {
      console.log('‚è≥ AI analysis in progress, skipping this cycle...');
      setTimeout(analysisLoop, 2000);
      return;
    }
    
    try {
      const now = Date.now();
      if (now - lastAnalysisTime < AI_CONFIG.analysisInterval) {
        setTimeout(analysisLoop, 1000);
        return;
      }
      
      lastAnalysisTime = now;
      aiAnalysisInProgress = true;
      console.log('\nüì∏ Capturing frame for analysis...');
      
      // Capture frame from RTSP stream
      const imageBase64 = await captureFrame(rtspUrl);
      
      if (imageBase64) {
        // Analyze with AI and wait for response
        const analysis = await analyzeImage(imageBase64);
        
        if (analysis) {
          console.log('ü§ñ AI Analysis Result:');
          console.log(`   ${analysis}`);
          
          // Announce the result (you could add text-to-speech here)
          announceVisionResult(analysis);
        }
        
        // Wait for the full analysis interval before next capture
        console.log(`‚è±Ô∏è  Waiting ${AI_CONFIG.analysisInterval}ms before next analysis...`);
        aiAnalysisInProgress = false;
        setTimeout(analysisLoop, AI_CONFIG.analysisInterval);
      } else {
        // If capture failed, retry sooner
        console.log('üîÑ Frame capture failed, retrying in 2 seconds...');
        aiAnalysisInProgress = false;
        setTimeout(analysisLoop, 2000);
      }
      
    } catch (error) {
      console.error('‚ùå Vision analysis error:', error.message);
      // On error, retry sooner
      aiAnalysisInProgress = false;
      setTimeout(analysisLoop, 2000);
    }
  };
  
  analysisLoop();
}

// Announce vision results (no automatic gestures)
function announceVisionResult(analysis) {
  console.log('üì¢ Vision Announcement:', analysis);
  
  // Here you could integrate with text-to-speech
  // For now, we'll just log it
  const timestamp = new Date().toLocaleTimeString();
  console.log(`üïê [${timestamp}] Camera sees: ${analysis}`);
  
  // No automatic gestures - only respond to user questions
  console.log('ü§ñ Camera is observing... Press "V" to ask a question!');
}

// Start voice recording for user questions
async function startVoiceRecording() {
  if (voiceRecordingActive) {
    console.log('üé§ Voice recording already active');
    return;
  }
  
  voiceRecordingActive = true;
  console.log('üé§ Starting voice recording... Speak your question now!');
  console.log('üé§ Speak clearly into the camera microphone...');
  
  try {
    // Use camera microphone recording (which we know works well)
    const audioFile = await recordAudioFromCamera();
    if (audioFile) {
      await processVoiceInput(null, audioFile);
    } else {
      console.log('‚ùå No audio recorded from camera microphone');
    }
  } catch (error) {
    console.log('‚ùå Camera microphone recording failed:', error.message);
  } finally {
    // Always reset the flag when done
    voiceRecordingActive = false;
    console.log('üé§ Voice recording session ended');
  }
}

// Start local microphone recording
function startLocalRecording() {
  console.log('üé§ Starting local microphone recording...');
  console.log('üé§ Recording config:', {
    sampleRate: VOICE_CONFIG.sampleRate,
    threshold: VOICE_CONFIG.threshold,
    silence: VOICE_CONFIG.silenceTimeout,
    duration: VOICE_CONFIG.recordingDuration
  });
  
  const recording = record.record({
    sampleRateHertz: VOICE_CONFIG.sampleRate,
    threshold: VOICE_CONFIG.threshold,
    silence: VOICE_CONFIG.silenceTimeout,
    recordProgram: 'rec' // Use 'rec' command for recording
  });
  
  const audioChunks = [];
  
  recording.stream()
    .on('data', (chunk) => {
      audioChunks.push(chunk);
      console.log('üé§ Audio chunk received:', chunk.length, 'bytes');
    })
    .on('end', async () => {
      console.log('üé§ Local voice recording completed');
      console.log('üé§ Total audio chunks:', audioChunks.length);
      voiceRecordingActive = false;
      
      if (audioChunks.length > 0) {
        const audioBuffer = Buffer.concat(audioChunks);
        console.log('üé§ Total audio buffer size:', audioBuffer.length, 'bytes');
        await processVoiceInput(audioBuffer);
      } else {
        console.log('‚ö†Ô∏è  No audio data recorded');
      }
    })
    .on('error', (error) => {
      console.error('‚ùå Local voice recording error:', error.message);
      voiceRecordingActive = false;
    });
  
  currentAudioStream = recording;
  
  // Stop recording after maximum duration
  setTimeout(() => {
    if (voiceRecordingActive) {
      console.log('‚è∞ Voice recording timeout reached');
      recording.stop();
    }
  }, VOICE_CONFIG.recordingDuration);
}

// Process voice input and convert to text
async function processVoiceInput(audioBuffer, audioFile = null) {
  try {
    console.log('üîä Processing voice input...');
    
    let tempAudioFile = audioFile;
    
    if (audioBuffer && !audioFile) {
      // Save audio buffer to temporary file
      tempAudioFile = './temp_voice.wav';
      fs.writeFileSync(tempAudioFile, audioBuffer);
    }
    
    // Convert speech to text using a simple approach
    // For now, we'll use a placeholder - you can integrate with Whisper API or similar
    const userQuestion = await convertSpeechToText(tempAudioFile);
    
    if (userQuestion) {
      console.log('üé§ User question:', userQuestion);
      await handleUserQuestion(userQuestion);
    }
    
    // Clean up
    if (tempAudioFile && fs.existsSync(tempAudioFile)) {
      fs.unlinkSync(tempAudioFile);
    }
    
  } catch (error) {
    console.error('‚ùå Voice processing error:', error.message);
  }
}

// Convert speech to text using real speech recognition
async function convertSpeechToText(audioFile) {
  console.log('üî§ *** ENTERING convertSpeechToText function ***');
  console.log('üî§ Converting speech to text...');
        console.log('      üìÅ Input audio file:', audioFile);
      console.log('      üìÅ Input file exists:', fs.existsSync(audioFile));
      console.log('      üìÅ Input file size:', fs.existsSync(audioFile) ? fs.statSync(audioFile).size : 'N/A', 'bytes');
      
      try {
        // Check if the input file is already WAV format
        const isWavFile = audioFile.toLowerCase().endsWith('.wav');
        let wavFile = audioFile;
        
        if (!isWavFile) {
          // Convert AAC to WAV format for better compatibility
          wavFile = audioFile.replace('.g711a', '.wav');
          console.log('      üîÑ Converting AAC to WAV:', wavFile);
      
          // Use ffmpeg to convert AAC to WAV
          const { spawn } = require('child_process');
          console.log('      üîß FFmpeg command:', 'ffmpeg', '-f', 'aac', '-i', audioFile, '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', '-y', wavFile);
          
          const ffmpeg = spawn('ffmpeg', [
            '-f', 'aac',  // Force AAC format
            '-i', audioFile,
            '-acodec', 'pcm_s16le',
            '-ar', '16000',
            '-ac', '1',
            '-y', // Overwrite
            wavFile
          ]);
        
        let stderr = '';
        let stdout = '';
        ffmpeg.stderr.on('data', (data) => {
          stderr += data.toString();
        });
        ffmpeg.stdout.on('data', (data) => {
          stdout += data.toString();
        });
    
            await new Promise((resolve, reject) => {
          ffmpeg.on('close', (code) => {
            console.log('      üîß FFmpeg exit code:', code);
            console.log('      üîß FFmpeg stdout:', stdout);
            console.log('      üîß FFmpeg stderr:', stderr);
            if (code === 0) {
              console.log('      ‚úÖ Audio converted to WAV format');
              // Add a small delay to ensure file system sync
              setTimeout(() => {
                console.log('      ‚è±Ô∏è  Waiting for file system sync...');
                resolve();
              }, 100);
            } else {
              console.log('      ‚ùå FFmpeg conversion failed with code', code);
              reject(new Error(`FFmpeg conversion failed with code ${code}`));
            }
          });
      
      ffmpeg.on('error', (error) => {
        console.log('      ‚ùå FFmpeg error:', error.message);
        reject(error);
      });
    });
        } else {
          console.log('      ‚úÖ Input file is already WAV format, no conversion needed');
        }
    
        // Try to use whisper-node for speech recognition
        try {
          // Capture working directory BEFORE loading whisper-node (which changes it)
          const originalCwd = process.cwd();
          console.log('      üìÅ Original working directory:', originalCwd);
          
          console.log('      üß† Loading Whisper model...');
          const { whisper } = require('whisper-node');
          
          console.log('      üé§ Starting speech recognition...');
          
          // Use absolute path to ensure we're looking in the right directory
          const absoluteWavPath = require('path').resolve(originalCwd, wavFile);
          console.log('      üìÅ Absolute WAV file path:', absoluteWavPath);
          console.log('      üìÅ WAV file exists:', fs.existsSync(absoluteWavPath));
          if (fs.existsSync(absoluteWavPath)) {
            console.log('      üìÅ WAV file size:', fs.statSync(absoluteWavPath).size, 'bytes');
          }
      
                // Add timeout to prevent hanging
          const whisperPromise = whisper(absoluteWavPath, {
            language: 'en',
            modelName: 'tiny' // Use the tiny model that we know works
          });
      
      const timeoutPromise = new Promise((_, reject) => {
        setTimeout(() => reject(new Error('Whisper timeout after 10 seconds')), 10000);
      });
      
      const result = await Promise.race([whisperPromise, timeoutPromise]);
      
            console.log('      üìä Whisper result:', result);
      console.log('      üìä Whisper result type:', typeof result);
      
      // Clean up the temporary WAV file
      if (fs.existsSync(absoluteWavPath)) {
        fs.unlinkSync(absoluteWavPath);
        console.log('      üßπ Cleaned up WAV file');
      }
      
      // Extract text from Whisper result (it returns an array of segments)
      let transcribedText = '';
      if (result && Array.isArray(result)) {
        transcribedText = result.map(segment => segment.speech).join(' ').trim();
        console.log('      üìä Extracted text from segments:', transcribedText);
      } else if (result && result.text) {
        transcribedText = result.text;
        console.log('      üìä Using result.text:', transcribedText);
      }
      
      if (transcribedText && transcribedText.length > 0) {
        console.log('üé§ Transcribed text:', transcribedText);
        return transcribedText;
      } else {
        console.log('‚ö†Ô∏è  No speech detected or recognition failed');
        return 'No speech detected';
      }
      
    } catch (whisperError) {
      console.log('‚ùå Whisper error:', whisperError.message);
      console.log('      üîç Whisper error details:', whisperError);
      
      // Fallback: analyze audio characteristics
      try {
        console.log('      üìä Falling back to audio analysis...');
        const { spawn } = require('child_process');
        
        // Use ffmpeg to analyze audio characteristics
        const analyze = spawn('ffmpeg', [
          '-i', wavFile,
          '-af', 'volumedetect',
          '-f', 'null',
          '-'
        ], { stdio: 'pipe' });
        
        let stderr = '';
        analyze.stderr.on('data', (data) => {
          stderr += data.toString();
        });
        
        await new Promise((resolve, reject) => {
          analyze.on('close', (code) => {
            if (code === 0 || code === 1) { // ffmpeg returns 1 for analysis
              console.log('      üìä Audio analysis completed');
              resolve();
            } else {
              reject(new Error(`Audio analysis failed`));
            }
          });
        });
        
        // Clean up the temporary WAV file
        if (fs.existsSync(wavFile)) {
          fs.unlinkSync(wavFile);
        }
        
        // Check if there's significant audio content
        if (stderr.includes('mean_volume') && !stderr.includes('mean_volume: -inf dB')) {
          console.log('üé§ Audio detected with speech content');
          return 'Speech detected but transcription unavailable';
        } else {
          console.log('‚ö†Ô∏è  No significant audio content detected');
          return 'No speech detected';
        }
        
      } catch (fallbackError) {
        console.log('‚ùå Fallback analysis also failed:', fallbackError.message);
        return 'Audio recorded but could not be transcribed';
      }
    }
    
  } catch (error) {
    console.log('‚ùå Speech recognition error:', error.message);
    console.log('      üîç Error details:', error);
    return 'Audio recorded but speech recognition failed';
  }
}

// Alternative: Capture audio from RTSP stream
async function captureAudioFromRTSP(rtspUrl) {
  return new Promise((resolve, reject) => {
    try {
      const outputPath = './temp_audio.wav';
      
      // Use ffmpeg to extract audio from RTSP stream
      const ffmpegArgs = [
        '-i', rtspUrl,
        '-vn', // No video
        '-acodec', 'pcm_s16le', // Audio codec
        '-ar', '16000', // Sample rate
        '-ac', '1', // Mono
        '-t', '5', // 5 seconds duration
        '-y', // Overwrite
        outputPath
      ];
      
      console.log('      üéµ Capturing audio from RTSP...');
      
      const ffmpeg = spawn('ffmpeg', ffmpegArgs, {
        stdio: ['ignore', 'pipe', 'pipe']
      });
      
      let stderr = '';
      
      ffmpeg.stderr.on('data', (data) => {
        stderr += data.toString();
      });
      
      ffmpeg.on('close', (code) => {
        if (code === 0 && fs.existsSync(outputPath)) {
          console.log('      ‚úÖ Audio captured successfully');
          resolve(outputPath);
        } else {
          console.error('      ‚ùå Audio capture failed:', stderr);
          reject(new Error(`Audio capture failed with code ${code}`));
        }
      });
      
      ffmpeg.on('error', (error) => {
        console.error('      ‚ùå Audio capture error:', error.message);
        reject(error);
      });
      
    } catch (error) {
      reject(error);
    }
  });
}

// Test microphone and speaker with record and playback using Amcrest API
async function testMicrophoneAndSpeaker() {
  console.log('\nüé§ Testing microphone and speaker using Amcrest API...');
  
  try {
    // Step 1: Record audio from camera microphone using getAudio
    console.log('üìπ Recording 5 seconds of audio from camera microphone...');
    console.log('üé§ Speak something into the camera microphone now!');
    
    const audioFile = await recordAudioFromCamera();
    
    if (audioFile && fs.existsSync(audioFile)) {
      console.log('‚úÖ Audio recording completed!');
      console.log(`üìÅ Audio file saved: ${audioFile} (${fs.statSync(audioFile).size} bytes)`);
      
      // Step 2: Play back the recorded audio through camera speaker
      console.log('üîä Playing back recorded audio through camera speaker...');
      console.log('üîß DEBUG: About to call playAudioThroughCamera function');
      await playAudioThroughCamera(audioFile);
      console.log('üîß DEBUG: playAudioThroughCamera completed');
      
      // Step 3: Try to convert speech to text
      console.log('üé§ Attempting speech-to-text conversion...');
      console.log('üîß DEBUG: About to call convertSpeechToText function');
      try {
        const transcribedText = await convertSpeechToText(audioFile);
        console.log('üìù Transcribed text:', transcribedText);
        console.log('üîß DEBUG: Speech-to-text completed successfully');
      } catch (sttError) {
        console.log('‚ö†Ô∏è  Speech-to-text failed:', sttError.message);
        console.log('üîß DEBUG: Speech-to-text error details:', sttError);
      }
      
      // Step 4: Clean up
      if (fs.existsSync(audioFile)) {
        fs.unlinkSync(audioFile);
        console.log('üßπ Cleaned up temporary audio file');
      }
    } else {
      console.log('‚ö†Ô∏è  No audio file was created or file is empty');
    }
    
  } catch (error) {
    console.error('‚ùå Microphone/speaker test failed:', error.message);
  }
}

// Record audio from camera using Amcrest getAudio API
async function recordAudioFromCamera() {
  return new Promise((resolve, reject) => {
    try {
      const http = require('http');
      const crypto = require('crypto');
      const outputPath = './temp_audio.g711a';
      
      // First request to get digest challenge
      const initialOptions = {
        hostname: cameraConfig.hostname,
        port: cameraConfig.port,
        path: '/cgi-bin/audio.cgi?action=getAudio&httptype=singlepart&channel=1',
        method: 'GET',
        headers: {
          'User-Agent': 'Amcrest-Camera-Client/1.0'
        }
      };
      
      console.log('      üéµ Recording audio from camera microphone...');
      console.log(`      üîê Auth: ${cameraConfig.username}:${cameraConfig.password}`);
      console.log(`      üåê URL: http://${cameraConfig.hostname}:${cameraConfig.port}${initialOptions.path}`);
      
      const initialReq = http.request(initialOptions, (res) => {
        console.log(`      üì° Initial HTTP Response: ${res.statusCode}`);
        
        if (res.statusCode === 401 && res.headers['www-authenticate']) {
          // Parse digest challenge
          const authHeader = res.headers['www-authenticate'];
          console.log(`      üîê Digest challenge: ${authHeader}`);
          
          // Extract digest parameters
          const realm = authHeader.match(/realm="([^"]+)"/)?.[1];
          const nonce = authHeader.match(/nonce="([^"]+)"/)?.[1];
          const qop = authHeader.match(/qop="([^"]+)"/)?.[1];
          const opaque = authHeader.match(/opaque="([^"]+)"/)?.[1];
          
          if (realm && nonce) {
            // Generate digest response
            const cnonce = crypto.randomBytes(16).toString('hex');
            const nc = '00000001';
            const uri = initialOptions.path;
            const method = 'GET';
            
            // Calculate digest response
            const ha1 = crypto.createHash('md5').update(`${cameraConfig.username}:${realm}:${cameraConfig.password}`).digest('hex');
            const ha2 = crypto.createHash('md5').update(`${method}:${uri}`).digest('hex');
            
            let response;
            if (qop === 'auth') {
              response = crypto.createHash('md5').update(`${ha1}:${nonce}:${nc}:${cnonce}:${qop}:${ha2}`).digest('hex');
            } else {
              response = crypto.createHash('md5').update(`${ha1}:${nonce}:${ha2}`).digest('hex');
            }
            
            // Build digest authorization header
            let digestAuth = `Digest username="${cameraConfig.username}", realm="${realm}", nonce="${nonce}", uri="${uri}", response="${response}"`;
            if (opaque) digestAuth += `, opaque="${opaque}"`;
            if (qop) digestAuth += `, qop=${qop}, nc=${nc}, cnonce="${cnonce}"`;
            
            console.log(`      üîê Digest auth: ${digestAuth}`);
            
            // Make authenticated request
            const authOptions = {
              hostname: cameraConfig.hostname,
              port: cameraConfig.port,
              path: '/cgi-bin/audio.cgi?action=getAudio&httptype=singlepart&channel=1',
              method: 'GET',
              headers: {
                'Authorization': digestAuth,
                'User-Agent': 'Amcrest-Camera-Client/1.0'
              }
            };
            
            const authReq = http.request(authOptions, (authRes) => {
              console.log(`      üì° Authenticated HTTP Response: ${authRes.statusCode}`);
              console.log(`      üì° Headers:`, authRes.headers);
              
              if (authRes.statusCode === 200) {
                const fileStream = fs.createWriteStream(outputPath);
                let audioData = Buffer.alloc(0);
                
                authRes.on('data', (chunk) => {
                  audioData = Buffer.concat([audioData, chunk]);
                });
                
                // Stop recording after 5 seconds
                setTimeout(() => {
                  authReq.destroy();
                  console.log('      ‚è±Ô∏è  Recording stopped after 5 seconds');
                  
                  if (audioData.length > 0) {
                    fileStream.write(audioData);
                    fileStream.end();
                    console.log(`      ‚úÖ Audio recording completed! (${audioData.length} bytes)`);
                    resolve(outputPath);
                  } else {
                    fileStream.end();
                    console.log('      ‚ö†Ô∏è  No audio data received');
                    reject(new Error('No audio data received'));
                  }
                }, 5000);
                
              } else {
                console.error('      ‚ùå Audio recording failed:', authRes.statusCode);
                reject(new Error(`Audio recording failed with status ${authRes.statusCode}`));
              }
            });
            
            authReq.on('error', (error) => {
              console.error('      ‚ùå Authenticated request error:', error.message);
              reject(error);
            });
            
            authReq.end();
            
          } else {
            console.error('      ‚ùå Could not parse digest challenge');
            reject(new Error('Could not parse digest challenge'));
          }
          
        } else {
          console.error('      ‚ùå Unexpected response:', res.statusCode);
          reject(new Error(`Unexpected response: ${res.statusCode}`));
        }
      });
      
      initialReq.on('error', (error) => {
        console.error('      ‚ùå Initial request error:', error.message);
        reject(error);
      });
      
      initialReq.end();
      
    } catch (error) {
      reject(error);
    }
  });
}

// Play audio through camera speaker using Amcrest postAudio API
async function playAudioThroughCamera(audioFile) {
  return new Promise((resolve, reject) => {
    // Add timeout to prevent hanging
    const timeout = setTimeout(() => {
      console.log('      ‚è±Ô∏è  Audio playback timeout after 10 seconds');
      resolve(); // Don't reject, just resolve to continue
    }, 10000);
    try {
      const http = require('http');
      const crypto = require('crypto');
      
      // Read the audio file
      const audioData = fs.readFileSync(audioFile);
      console.log(`      üìÅ Audio file size: ${audioData.length} bytes`);
      
      // First request to get digest challenge
      const initialOptions = {
        hostname: cameraConfig.hostname,
        port: cameraConfig.port,
        path: '/cgi-bin/audio.cgi?action=postAudio&httptype=singlepart&channel=1',
        method: 'POST',
        headers: {
          'Content-Type': 'Audio/AAC',
          'Content-Length': audioData.length,
          'User-Agent': 'Amcrest-Camera-Client/1.0'
        }
      };
      
      console.log('      üì° Sending audio to camera speaker...');
      
      const initialReq = http.request(initialOptions, (res) => {
        console.log(`      üì° Initial HTTP Response: ${res.statusCode}`);
        
        if (res.statusCode === 401 && res.headers['www-authenticate']) {
          // Parse digest challenge
          const authHeader = res.headers['www-authenticate'];
          console.log(`      üîê Digest challenge: ${authHeader}`);
          
          // Extract digest parameters
          const realm = authHeader.match(/realm="([^"]+)"/)?.[1];
          const nonce = authHeader.match(/nonce="([^"]+)"/)?.[1];
          const qop = authHeader.match(/qop="([^"]+)"/)?.[1];
          const opaque = authHeader.match(/opaque="([^"]+)"/)?.[1];
          
          if (realm && nonce) {
            // Generate digest response
            const cnonce = crypto.randomBytes(16).toString('hex');
            const nc = '00000001';
            const uri = initialOptions.path;
            const method = 'POST';
            
            // Calculate digest response
            const ha1 = crypto.createHash('md5').update(`${cameraConfig.username}:${realm}:${cameraConfig.password}`).digest('hex');
            const ha2 = crypto.createHash('md5').update(`${method}:${uri}`).digest('hex');
            
            let response;
            if (qop === 'auth') {
              response = crypto.createHash('md5').update(`${ha1}:${nonce}:${nc}:${cnonce}:${qop}:${ha2}`).digest('hex');
            } else {
              response = crypto.createHash('md5').update(`${ha1}:${nonce}:${ha2}`).digest('hex');
            }
            
            // Build digest authorization header
            let digestAuth = `Digest username="${cameraConfig.username}", realm="${realm}", nonce="${nonce}", uri="${uri}", response="${response}"`;
            if (opaque) digestAuth += `, opaque="${opaque}"`;
            if (qop) digestAuth += `, qop=${qop}, nc=${nc}, cnonce="${cnonce}"`;
            
            console.log(`      üîê Digest auth: ${digestAuth}`);
            
            // Make authenticated request
            const authOptions = {
              hostname: cameraConfig.hostname,
              port: cameraConfig.port,
              path: '/cgi-bin/audio.cgi?action=postAudio&httptype=singlepart&channel=1',
              method: 'POST',
              headers: {
                'Content-Type': 'Audio/AAC',
                'Content-Length': audioData.length,
                'Authorization': digestAuth,
                'User-Agent': 'Amcrest-Camera-Client/1.0'
              }
            };
            
            const authReq = http.request(authOptions, (authRes) => {
              let responseData = '';
              
              authRes.on('data', (chunk) => {
                responseData += chunk.toString();
              });
              
              authRes.on('end', () => {
                console.log(`      üì° Authenticated HTTP Response: ${authRes.statusCode}`);
                console.log(`      üì° Response: ${responseData}`);
                
                if (authRes.statusCode === 200 && responseData.includes('OK')) {
                  console.log('      ‚úÖ Audio sent to camera speaker successfully!');
                } else {
                  console.log('      ‚ö†Ô∏è  Audio playback may not be supported or failed');
                }
                clearTimeout(timeout);
                resolve();
              });
            });
            
            authReq.on('error', (err) => {
              console.log('      ‚ùå Authenticated request error:', err.message);
              console.log('      üí° Camera may not support audio playback via HTTP');
              clearTimeout(timeout);
              resolve(); // Don't reject, just note that it's not supported
            });
            
            authReq.write(audioData);
            authReq.end();
            
                  } else {
          console.error('      ‚ùå Could not parse digest challenge');
          clearTimeout(timeout);
          resolve(); // Don't reject, just note the issue
        }
          
        } else {
          console.log(`      üì° Unexpected response: ${res.statusCode}`);
          clearTimeout(timeout);
          resolve(); // Don't reject, just note the issue
        }
      });
      
      initialReq.on('error', (err) => {
        console.log('      ‚ùå Initial request error:', err.message);
        console.log('      üí° Camera may not support audio playback via HTTP');
        clearTimeout(timeout);
        resolve(); // Don't reject, just note that it's not supported
      });
      
      initialReq.write(audioData);
      initialReq.end();
      
    } catch (error) {
      console.log('      ‚ùå Audio playback error:', error.message);
      clearTimeout(timeout);
      resolve(); // Don't reject, just note the error
    }
  });
}

// Handle user question with AI vision
async function handleUserQuestion(question) {
  try {
    console.log('ü§ñ Processing question with AI vision...');
    
    // Capture current frame
    const rtspUrl = `rtsp://${cameraConfig.username}:${cameraConfig.password}@${cameraConfig.hostname}:${cameraConfig.port}/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif`;
    const imageBase64 = await captureFrame(rtspUrl);
    
    if (imageBase64) {
      // Send question and image to AI
      const response = await ollama.chat({
        model: AI_CONFIG.model,
        messages: [
          {
            role: 'system',
            content: 'You are a helpful AI assistant that answers questions. For questions about what you see in the image, use the image. For general knowledge questions, use your knowledge. Always respond with "YES" if the answer is affirmative, "NO" if negative, or provide a brief description. Be concise and direct.'
          },
          {
            role: 'user',
            content: question,
            images: [imageBase64]
          }
        ],
        stream: false
      });
      
      const answer = response.message.content;
      console.log('ü§ñ AI Answer:', answer);
      
      // Determine gesture based on answer
      const answerLower = answer.toLowerCase();
      if (answerLower.includes('yes') || answerLower.includes('affirmative') || answerLower.includes('true')) {
        console.log('‚úÖ Answer is YES - Camera will nod in response');
        gestureInProgress = true;
        gestureYes(() => {
          console.log('‚úÖ Camera nodded "YES" to your question');
          gestureInProgress = false;
        });
      } else if (answerLower.includes('no') || answerLower.includes('negative') || answerLower.includes('false')) {
        console.log('‚ùå Answer is NO - Camera will shake in response');
        gestureInProgress = true;
        gestureNo(() => {
          console.log('‚úÖ Camera shook "NO" to your question');
          gestureInProgress = false;
        });
      } else {
        console.log('ü§î Ambiguous answer - Camera will not gesture');
        console.log('üí° Try asking a yes/no question for a gesture response');
      }
    }
    
  } catch (error) {
    console.error('‚ùå Question processing error:', error.message);
  }
}

// Start voice interaction system
function startVoiceInteraction() {
  console.log('\nüé§ Voice Interaction System Ready!');
  console.log('ü§ñ Camera is observing and waiting for your questions...');
  console.log('üí° Press "V" for voice question, "T" for text question, "M" for mic test, "Q" to quit');
  
  // Set up keyboard listener
  process.stdin.setRawMode(true);
  process.stdin.resume();
  process.stdin.setEncoding('utf8');
  
  process.stdin.on('data', async (key) => {
    // Handle Ctrl+C (ASCII 3)
    if (key === '\u0003') {
      console.log('\nüõë Ctrl+C detected - stopping application...');
      process.stdin.setRawMode(false);
      process.stdin.pause();
      stopVisionAnalysis();
      cam.stop(() => {
        console.log('üëã Demo stopped. Goodbye!');
        process.exit(0);
      });
      return;
    }
    
    if (key === 'v' || key === 'V') {
      console.log('\nüé§ Voice recording triggered! Speak your question now...');
      startVoiceRecording();
    } else if (key === 't' || key === 'T') {
      console.log('\nüìù Text input mode - Type your question and press Enter:');
      process.stdin.setRawMode(false);
      process.stdin.setEncoding('utf8');
      
      process.stdin.once('data', async (question) => {
        const cleanQuestion = question.toString().trim();
        console.log('üé§ Text question:', cleanQuestion);
        await handleUserQuestion(cleanQuestion);
        
        // Return to raw mode for keyboard shortcuts
        process.stdin.setRawMode(true);
        process.stdin.setEncoding('utf8');
        console.log('\nüí° Press "V" for voice, "T" for text, "M" for mic test, "Q" to quit');
      });
    } else if (key === 'm' || key === 'M') {
      console.log('\nüé§ Starting microphone and speaker test...');
      await testMicrophoneAndSpeaker();
      console.log('\nüí° Press "V" for voice, "T" for text, "M" for mic test, "Q" to quit');
    } else if (key === 'q' || key === 'Q') {
      console.log('\nüëã Quitting...');
      process.stdin.setRawMode(false);
      process.stdin.pause();
      stopVisionAnalysis();
      process.exit(0);
    }
  });
}

// Stop AI vision analysis
function stopVisionAnalysis() {
  visionAnalysisActive = false;
  aiAnalysisInProgress = false;
  gestureInProgress = false;
  voiceRecordingActive = false;
  if (currentAudioStream) {
    currentAudioStream.stop();
    currentAudioStream = null;
  }
  console.log('‚èπÔ∏è  AI Vision Analysis stopped');
}

const cam = new Cam(cameraConfig, async function(err) {
  if (err) {
    console.error('‚ùå Failed to connect to camera:', err.message);
    process.exit(1);
  }
  
  console.log('‚úÖ Successfully connected to camera!');
  
  // Initialize Ollama
  const ollamaReady = await initializeOllama();
  if (!ollamaReady) {
    console.log('‚ö†Ô∏è  Continuing without AI vision capabilities...');
  }
  
  // Get camera information
  console.log('\nüì∑ Camera Information:');
  console.log(`   Manufacturer: ${cam.hostname}`);
  console.log(`   Model: ${cam.name}`);
  console.log(`   Hardware ID: ${cam.hardwareId}`);
  console.log(`   Location: ${cam.location}`);
  
  // Get PTZ configuration
  cam.getConfigurations((err, configs) => {
    if (err) {
      console.error('‚ùå Failed to get PTZ configurations:', err.message);
      return;
    }
    
    console.log('\nüéÆ PTZ Configurations found:', configs.length);
    
    // Get stream URI
    cam.getStreamUri({ protocol: 'RTSP' }, (err, res) => {
      if (!err) {
        console.log('\nüì∫ RTSP Stream URL:', res.uri);
        
        // Add authentication to RTSP URL
        const authenticatedRtspUrl = res.uri.replace('rtsp://', `rtsp://${cameraConfig.username}:${cameraConfig.password}@`);
        console.log('üîê Authenticated RTSP URL:', authenticatedRtspUrl);
        
        // Start AI vision analysis if Ollama is ready
        if (ollamaReady) {
          console.log('\nüöÄ Starting AI Vision Analysis...');
          startVisionAnalysis(authenticatedRtspUrl);
        }
        
        // Start voice interaction system
        startVoiceInteraction();
        
        // Start PTZ demo
        // startPersonalityDemo();
      } else {
        console.error('‚ùå Failed to get RTSP URL:', err.message);
        // startPersonalityDemo();
      }
    });
  });
});

// Check what audio capabilities the camera supports
function checkAudioCapabilities(callback) {
  console.log('\nüîä Checking Audio Capabilities...');
  
  let audioChecks = 0;
  const totalChecks = 3;
  
  function checkComplete() {
    audioChecks++;
    if (audioChecks >= totalChecks) {
      console.log('   ‚úÖ Audio capability check completed');
      if (callback) callback();
    }
  }
  
  // Check for audio sources (microphones)
  cam.getAudioSources((err, sources) => {
    if (!err && sources && sources.length > 0) {
      console.log('   üé§ Audio Sources found:', sources.length);
      sources.forEach((source, index) => {
        console.log(`      ${index + 1}. ${source.name || 'Unknown'} (${source.token})`);
        if (source.configurations) {
          console.log(`         Configurations: ${source.configurations.length}`);
        }
      });
    } else {
      console.log('   ‚ùå No audio sources found or error:', err ? err.message : 'No sources');
    }
    checkComplete();
  });
  
  // Check for audio outputs (speakers)
  cam.getAudioOutputs((err, outputs) => {
    if (!err && outputs && outputs.length > 0) {
      console.log('   üîä Audio Outputs found:', outputs.length);
      outputs.forEach((output, index) => {
        console.log(`      ${index + 1}. ${output.name || 'Unknown'} (${output.token})`);
        if (output.configurations) {
          console.log(`         Configurations: ${output.configurations.length}`);
        }
      });
    } else {
      console.log('   ‚ùå No audio outputs found or error:', err ? err.message : 'No outputs');
    }
    checkComplete();
  });
  
  // Check for audio encoder configurations
  cam.getAudioEncoderConfigurations((err, configs) => {
    if (!err && configs && configs.length > 0) {
      console.log('   üéµ Audio Encoder Configurations found:', configs.length);
      configs.forEach((config, index) => {
        console.log(`      ${index + 1}. ${config.name || 'Unknown'} (${config.token})`);
        if (config.encoding) {
          console.log(`         Encoding: ${config.encoding}`);
        }
        if (config.bitrate) {
          console.log(`         Bitrate: ${config.bitrate}`);
        }
        if (config.sampleRate) {
          console.log(`         Sample Rate: ${config.sampleRate}`);
        }
      });
    } else {
      console.log('   ‚ùå No audio encoder configurations found or error:', err ? err.message : 'No configs');
    }
    checkComplete();
  });
}

// function startPTZDemo() {
//   console.log('\nüé¨ Starting PTZ Demo...');
//   console.log('‚è±Ô∏è  Each movement will last 2 seconds');
  
//   // Demo sequence
//   const demoSequence = [
//     { name: 'Pan Right', x: 0.3, y: 0.0, zoom: 0.0 },
//     { name: 'Pan Left', x: -0.3, y: 0.0, zoom: 0.0 },
//     { name: 'Tilt Up', x: 0.0, y: 0.3, zoom: 0.0 },
//     { name: 'Tilt Down', x: 0.0, y: -0.3, zoom: 0.0 },
//     // { name: 'Zoom In', x: 0.0, y: 0.0, zoom: 0.3 },
//     // { name: 'Zoom Out', x: 0.0, y: 0.0, zoom: -0.3 },
//     // { name: 'Diagonal Movement', x: 0.2, y: 0.2, zoom: 0.0 },
//     { name: 'Return to Center', x: 0.0, y: 0.0, zoom: 0.0 }
//   ];
  
//   let currentIndex = 0;
  
//   function executeNextMovement() {
//     if (currentIndex >= demoSequence.length) {
//       console.log('\n‚úÖ PTZ Demo completed!');
//       process.exit(0);
//     }
    
//     const movement = demoSequence[currentIndex];
//     console.log(`\nüîÑ ${movement.name}...`);
//     console.log(`   Pan: ${movement.x}, Tilt: ${movement.y}, Zoom: ${movement.zoom}`);
    
//     cam.continuousMove(movement, (err) => {
//       if (err) {
//         console.error(`‚ùå Failed to execute ${movement.name}:`, err.message);
//       } else {
//         console.log(`   ‚úÖ ${movement.name} started`);
//       }
//     });
    
//     // Stop movement after 2 seconds
//     setTimeout(() => {
//       cam.stop((err) => {
//         if (err) {
//           console.error(`‚ùå Failed to stop ${movement.name}:`, err.message);
//         } else {
//           console.log(`   ‚èπÔ∏è  ${movement.name} stopped`);
//         }
        
//         // Wait 1 second before next movement
//         setTimeout(() => {
//           currentIndex++;
//           executeNextMovement();
//         }, 1000);
//       });
//     }, 2000);
//   }
  
//   // Start the demo sequence
//   executeNextMovement();
// }

// Gesture functions for personality
function gestureYes(callback) {
  console.log('\nüôÇ Device says "YES" (nodding up and down)...');
  
  let nodCount = 0;
  const maxNods = 3;
  
  function performNod() {
    if (nodCount >= maxNods) {
      // Return to center
      cam.continuousMove({ x: 0.0, y: 0.0, zoom: 0.0 }, (err) => {
        if (err) {
          console.error('‚ùå Failed to return to center:', err.message);
        } else {
          console.log('   ‚úÖ Returned to center');
        }
        
        setTimeout(() => {
          cam.stop(() => {
            console.log('   üé≠ "Yes" gesture completed!');
            if (callback) callback();
          });
        }, 1000);
      });
      return;
    }
    
    console.log(`   Nod ${nodCount + 1}/${maxNods}: Tilting up...`);
    // Nod up - larger movement and longer duration
    cam.continuousMove({ x: 0.0, y: 0.3, zoom: 0.0 }, (err) => {
      if (err) {
        console.error('‚ùå Failed to nod up:', err.message);
        return;
      }
      
      setTimeout(() => {
        console.log(`   Nod ${nodCount + 1}/${maxNods}: Tilting down...`);
        // Nod down - larger movement and longer duration
        cam.continuousMove({ x: 0.0, y: -0.3, zoom: 0.0 }, (err) => {
          if (err) {
            console.error('‚ùå Failed to nod down:', err.message);
            return;
          }
          
          setTimeout(() => {
            nodCount++;
            performNod();
          }, 800);
        });
      }, 800);
    });
  }
  
  performNod();
}

function gestureNo(callback) {
  console.log('\nüòê Device says "NO" (shaking left and right)...');
  
  let shakeCount = 0;
  const maxShakes = 3;
  
  function performShake() {
    if (shakeCount >= maxShakes) {
      // Return to center
      cam.continuousMove({ x: 0.0, y: 0.0, zoom: 0.0 }, (err) => {
        if (err) {
          console.error('‚ùå Failed to return to center:', err.message);
        } else {
          console.log('   ‚úÖ Returned to center');
        }
        
        setTimeout(() => {
          cam.stop(() => {
            console.log('   üé≠ "No" gesture completed!');
            if (callback) callback();
          });
        }, 1000);
      });
      return;
    }
    
    console.log(`   Shake ${shakeCount + 1}/${maxShakes}: Panning left...`);
    // Shake left - larger movement and longer duration
    cam.continuousMove({ x: -0.3, y: 0.0, zoom: 0.0 }, (err) => {
      if (err) {
        console.error('‚ùå Failed to shake left:', err.message);
        return;
      }
      
      setTimeout(() => {
        console.log(`   Shake ${shakeCount + 1}/${maxShakes}: Panning right...`);
        // Shake right - larger movement and longer duration
        cam.continuousMove({ x: 0.3, y: 0.0, zoom: 0.0 }, (err) => {
          if (err) {
            console.error('‚ùå Failed to shake right:', err.message);
            return;
          }
          
          setTimeout(() => {
            shakeCount++;
            performShake();
          }, 800);
        });
      }, 800);
    });
  }
  
  performShake();
}

// Enhanced demo with personality gestures
function startPersonalityDemo() {
  console.log('\nüé≠ Starting Personality Demo...');
  console.log('ü§ñ Watch the device show some personality!');
  
  const personalitySequence = [
    { name: 'Pan Right', x: 0.3, y: 0.0, zoom: 0.0 },
    { name: 'Pan Left', x: -0.3, y: 0.0, zoom: 0.0 },
    { name: 'Tilt Up', x: 0.0, y: 0.3, zoom: 0.0 },
    { name: 'Tilt Down', x: 0.0, y: -0.3, zoom: 0.0 },
    { name: 'Return to Center', x: 0.0, y: 0.0, zoom: 0.0 }
  ];
  
  let currentIndex = 0;
  
  function executeNextMovement() {
    if (currentIndex >= personalitySequence.length) {
      // After basic movements, do personality gestures
      console.log('\nüé≠ Time for some personality!');
      
      // Do "yes" gesture
      gestureYes(() => {
        setTimeout(() => {
          // Do "no" gesture
          gestureNo(() => {
            console.log('\n‚úÖ Personality Demo completed!');
            console.log('ü§ñ AI Vision Analysis will continue running...');
            console.log('üí° Press Ctrl+C to stop the application');
          });
        }, 2000);
      });
      return;
    }
    
    const movement = personalitySequence[currentIndex];
    console.log(`\nüîÑ ${movement.name}...`);
    console.log(`   Pan: ${movement.x}, Tilt: ${movement.y}, Zoom: ${movement.zoom}`);
    
    cam.continuousMove(movement, (err) => {
      if (err) {
        console.error(`‚ùå Failed to execute ${movement.name}:`, err.message);
      } else {
        console.log(`   ‚úÖ ${movement.name} started`);
      }
    });
    
    // Stop movement after 2 seconds
    setTimeout(() => {
      cam.stop((err) => {
        if (err) {
          console.error(`‚ùå Failed to stop ${movement.name}:`, err.message);
        } else {
          console.log(`   ‚èπÔ∏è  ${movement.name} stopped`);
        }
        
        // Wait 1 second before next movement
        setTimeout(() => {
          currentIndex++;
          executeNextMovement();
        }, 1000);
      });
    }, 2000);
  }
  
  // Start the personality demo sequence
  executeNextMovement();
}

// Gesture functions for personality
function gestureYes(callback) {
  console.log('\nüôÇ Device says "YES" (nodding up and down)...');
  
  let nodCount = 0;
  const maxNods = 2;
  
  function performNod() {
    if (nodCount >= maxNods) {
      // Return to center
      cam.continuousMove({ x: 0.0, y: 0.0, zoom: 0.0 }, (err) => {
        if (err) {
          console.error('‚ùå Failed to return to center:', err.message);
        } else {
          console.log('   ‚úÖ Returned to center');
        }
        
        setTimeout(() => {
          cam.stop(() => {
            console.log('   üé≠ "Yes" gesture completed!');
            if (callback) callback();
          });
        }, 1000);
      });
      return;
    }
    
    console.log(`   Nod ${nodCount + 1}/${maxNods}: Tilting up...`);
    // Nod up - larger movement and longer duration
    cam.continuousMove({ x: 0.0, y: 0.3, zoom: 0.0 }, (err) => {
      if (err) {
        console.error('‚ùå Failed to nod up:', err.message);
        return;
      }
      
      setTimeout(() => {
        console.log(`   Nod ${nodCount + 1}/${maxNods}: Tilting down...`);
        // Nod down - larger movement and longer duration
        cam.continuousMove({ x: 0.0, y: -0.3, zoom: 0.0 }, (err) => {
          if (err) {
            console.error('‚ùå Failed to nod down:', err.message);
            return;
          }
          
          setTimeout(() => {
            nodCount++;
            performNod();
          }, 800);
        });
      }, 800);
    });
  }
  
  performNod();
}

function gestureNo(callback) {
  console.log('\nüòê Device says "NO" (shaking left and right)...');
  
  let shakeCount = 0;
  const maxShakes = 2;
  
  function performShake() {
    if (shakeCount >= maxShakes) {
      // Return to center
      cam.continuousMove({ x: 0.0, y: 0.0, zoom: 0.0 }, (err) => {
        if (err) {
          console.error('‚ùå Failed to return to center:', err.message);
        } else {
          console.log('   ‚úÖ Returned to center');
        }
        
        setTimeout(() => {
          cam.stop(() => {
            console.log('   üé≠ "No" gesture completed!');
            if (callback) callback();
          });
        }, 1000);
      });
      return;
    }
    
    console.log(`   Shake ${shakeCount + 1}/${maxShakes}: Panning left...`);
    // Shake left - larger movement and longer duration
    cam.continuousMove({ x: -0.3, y: 0.0, zoom: 0.0 }, (err) => {
      if (err) {
        console.error('‚ùå Failed to shake left:', err.message);
        return;
      }
      
      setTimeout(() => {
        console.log(`   Shake ${shakeCount + 1}/${maxShakes}: Panning right...`);
        // Shake right - larger movement and longer duration
        cam.continuousMove({ x: 0.3, y: 0.0, zoom: 0.0 }, (err) => {
          if (err) {
            console.error('‚ùå Failed to shake right:', err.message);
            return;
          }
          
          setTimeout(() => {
            shakeCount++;
            performShake();
          }, 800);
        });
      }, 800);
    });
  }
  
  performShake();
}

// Enhanced demo with personality gestures
function startPersonalityDemo() {
  console.log('\nüé≠ Starting Personality Demo...');
  console.log('ü§ñ Watch the device show some personality!');
  
  const personalitySequence = [
    { name: 'Pan Right', x: 0.3, y: 0.0, zoom: 0.0 },
    { name: 'Pan Left', x: -0.3, y: 0.0, zoom: 0.0 },
    { name: 'Tilt Up', x: 0.0, y: 0.3, zoom: 0.0 },
    { name: 'Tilt Down', x: 0.0, y: -0.3, zoom: 0.0 },
    { name: 'Return to Center', x: 0.0, y: 0.0, zoom: 0.0 }
  ];
  
  let currentIndex = 0;
  
  function executeNextMovement() {
    if (currentIndex >= personalitySequence.length) {
      // After basic movements, do personality gestures
      console.log('\nüé≠ Time for some personality!');
      
      // Do "yes" gesture
      gestureYes(() => {
        setTimeout(() => {
          // Do "no" gesture
          gestureNo(() => {
            console.log('\n‚úÖ Personality Demo completed!');
            console.log('ü§ñ AI Vision Analysis will continue running...');
            console.log('üí° Press Ctrl+C to stop the application');
          });
        }, 2000);
      });
      return;
    }
    
    const movement = personalitySequence[currentIndex];
    console.log(`\nüîÑ ${movement.name}...`);
    console.log(`   Pan: ${movement.x}, Tilt: ${movement.y}, Zoom: ${movement.zoom}`);
    
    cam.continuousMove(movement, (err) => {
      if (err) {
        console.error(`‚ùå Failed to execute ${movement.name}:`, err.message);
      } else {
        console.log(`   ‚úÖ ${movement.name} started`);
      }
    });
    
    // Stop movement after 2 seconds
    setTimeout(() => {
      cam.stop((err) => {
        if (err) {
          console.error(`‚ùå Failed to stop ${movement.name}:`, err.message);
        } else {
          console.log(`   ‚èπÔ∏è  ${movement.name} stopped`);
        }
        
        // Wait 1 second before next movement
        setTimeout(() => {
          currentIndex++;
          executeNextMovement();
        }, 1000);
      });
    }, 2000);
  }
  
  // Start the personality demo sequence
  executeNextMovement();
}

// Handle process termination
process.on('SIGINT', () => {
  console.log('\nüõë Stopping camera movements and AI vision...');
  stopVisionAnalysis();
  cam.stop(() => {
    console.log('üëã Demo stopped. Goodbye!');
    process.exit(0);
  });
});

process.on('SIGTERM', () => {
  console.log('\nüõë Stopping camera movements and AI vision...');
  stopVisionAnalysis();
  cam.stop(() => {
    console.log('üëã Demo stopped. Goodbye!');
    process.exit(0);
  });
});



